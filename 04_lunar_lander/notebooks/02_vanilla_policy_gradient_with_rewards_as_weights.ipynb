{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "d77fe799",
   "metadata": {},
   "source": [
    "# 03 Vanilla Policy Gradient with rewards as weights\n",
    "\n",
    "#### üëâüèΩ The policy network will have the following architecture:\n",
    "\n",
    "<img src=\"https://github.com/Paulescu/hands-on-rl/blob/main/04_lunar_lander/images/policy_network.svg?raw=True\" width=\"300\"/>\n",
    "\n",
    "#### üëâüèΩ And we will use the rewards to compute the weights in the policy gradient formula."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7a338bc0",
   "metadata": {},
   "outputs": [],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2\n",
    "%pylab inline\n",
    "%config InlineBackend.figure_format = 'svg'"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "be268c45",
   "metadata": {},
   "source": [
    "## Environment üöÄüåô"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "60b5127b",
   "metadata": {},
   "outputs": [],
   "source": [
    "ENV_NAME = 'LunarLander-v3'"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "df39e471",
   "metadata": {},
   "source": [
    "## Create agent and set hyperparameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "551fc934",
   "metadata": {},
   "outputs": [],
   "source": [
    "from src.vpg_agent import VPGAgent\n",
    "\n",
    "agent = VPGAgent(\n",
    "    env_name=ENV_NAME,\n",
    "    learning_rate=3e-3,\n",
    "    hidden_layers=[16, 16],\n",
    "    gradient_weights='rewards',\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1a78ce19",
   "metadata": {},
   "source": [
    "## Tensorboard logger to see training metrics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f13a9f45",
   "metadata": {},
   "outputs": [],
   "source": [
    "from src.utils import get_agent_id\n",
    "agent_id = get_agent_id(ENV_NAME)\n",
    "print(f'agent_id = {agent_id}')\n",
    "\n",
    "# tensorboard logger to see training curves\n",
    "from src.utils import get_logger, get_model_path\n",
    "logger = get_logger(env_name=ENV_NAME, agent_id=agent_id)\n",
    "\n",
    "# path to save policy network weights and hyperparameters\n",
    "model_path = get_model_path(env_name=ENV_NAME, agent_id=agent_id)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ec4f45d0",
   "metadata": {},
   "source": [
    "## Training üèãÔ∏è"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8fa8c1d6",
   "metadata": {},
   "outputs": [],
   "source": [
    "agent.train(\n",
    "    n_policy_updates=5000,\n",
    "    batch_size=256,\n",
    "    logger=logger,\n",
    "    model_path=model_path,\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "eb0efd20",
   "metadata": {},
   "source": [
    "## Evaluate the agent ‚è±Ô∏è"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2d8b4a82",
   "metadata": {},
   "outputs": [],
   "source": [
    "rewards, success = agent.evaluate(n_episodes=100)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b3c4c8b6",
   "metadata": {},
   "source": [
    "### Average reward"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "50ecdea6",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "reward_avg = np.array(rewards).mean()\n",
    "reward_std = np.array(rewards).std()\n",
    "print(f'Reward average {reward_avg:.2f}, std {reward_std:.2f}')\n",
    "\n",
    "success_rate = np.array(success).mean()\n",
    "print(f'Succes rate = {success_rate:.2%}')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f679dc71",
   "metadata": {},
   "source": [
    "## Reward distribution"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "839594d1",
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import pandas as pd\n",
    "\n",
    "fig, ax = plt.subplots(figsize = (10, 4))\n",
    "ax.set_title(\"Rewards\")    \n",
    "pd.Series(rewards).plot(kind='hist', bins=100)\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dc91cca4",
   "metadata": {},
   "source": [
    "## Let's see our agent in action üé¨"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "83b9d5b1",
   "metadata": {
    "lines_to_next_cell": 0
   },
   "outputs": [],
   "source": [
    "# Workaround for pygame error: \"error: No available video device\"\n",
    "# See https://stackoverflow.com/questions/15933493/pygame-error-no-available-video-device?rq=1\n",
    "# This is probably needed only for Linux\n",
    "import os\n",
    "os.environ[\"SDL_VIDEODRIVER\"] = \"dummy\"\n",
    "from src.viz import show_video\n",
    "\n",
    "show_video(agent, env, sleep_sec=0.01, seed=12345)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b12c5744",
   "metadata": {
    "lines_to_next_cell": 2
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "src-GwqnSlDl-py3.12",
   "language": "python",
   "name": "python3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
