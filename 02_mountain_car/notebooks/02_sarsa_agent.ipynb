{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "01e4bd19",
   "metadata": {},
   "source": [
    "# 02 SARSA agent"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1164d32c",
   "metadata": {},
   "outputs": [],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2\n",
    "%pylab inline\n",
    "%config InlineBackend.figure_format = 'svg'"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dd053f24",
   "metadata": {},
   "source": [
    "## Environment ðŸŒŽ"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4682c42b",
   "metadata": {},
   "outputs": [],
   "source": [
    "import gymnasium as gym\n",
    "env = gym.make('MountainCar-v0', max_episode_steps=1000)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bf1835f6",
   "metadata": {},
   "source": [
    "## SARSA agent ðŸ¤–"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bd5b3b20",
   "metadata": {},
   "outputs": [],
   "source": [
    "from src.sarsa_agent import SarsaAgent\n",
    "\n",
    "alpha = 0.1 # learning rate\n",
    "gamma = 0.9 # discount factor\n",
    "\n",
    "agent = SarsaAgent(env, alpha, gamma)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a4affb0d",
   "metadata": {},
   "source": [
    "## Train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "44a7cfb2",
   "metadata": {},
   "outputs": [],
   "source": [
    "from src.loops import train\n",
    "\n",
    "rewards, max_positions = train(\n",
    "    agent, env, n_episodes=10000, epsilon=0.1)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fe8ed8b0",
   "metadata": {},
   "source": [
    "## Plot train metrics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "448a467a",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "fig, ax = plt.subplots(figsize = (12, 4))\n",
    "ax.set_title(\"Rewards\")\n",
    "pd.Series(rewards).plot(kind='line')\n",
    "pd.Series(rewards).rolling(window=50).mean().plot(kind='line')\n",
    "plt.show()\n",
    "\n",
    "fig, ax = plt.subplots(figsize = (12, 4))\n",
    "ax.set_title(\"Max position\")    \n",
    "pd.Series(max_positions).plot(kind='line')\n",
    "pd.Series(max_positions).rolling(window=50).mean().plot(kind='line')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cc0d2204",
   "metadata": {},
   "source": [
    "## Evaluate"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "19c42631",
   "metadata": {},
   "outputs": [],
   "source": [
    "from src.loops import evaluate\n",
    "\n",
    "n_episodes = 1000\n",
    "eval_rewards, eval_max_positions = evaluate(\n",
    "    agent, env, n_episodes)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8627cc28",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "fig, ax = plt.subplots(figsize = (12, 4))\n",
    "ax.set_title(\"Rewards\")    \n",
    "pd.Series(eval_rewards).plot(kind='line')\n",
    "plt.show()\n",
    "\n",
    "fig, ax = plt.subplots(figsize = (12, 4))\n",
    "ax.set_title(\"Max position\")    \n",
    "pd.Series(eval_max_positions).plot(kind='line')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8889093e",
   "metadata": {},
   "source": [
    "## And the success rate of our trained `SarsaAgent` is..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7804a274",
   "metadata": {},
   "outputs": [],
   "source": [
    "n_completed = sum([1 if m > 0.5 else 0 for m in eval_max_positions])\n",
    "print(f'{n_completed} success out of {n_episodes} attempts')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "973fbda9",
   "metadata": {},
   "source": [
    "## Let's plot the agent's policy\n",
    "\n",
    "#### ðŸ‘‰This is the agent's greedy policy.\n",
    "\n",
    "#### ðŸ‘‰The greedy policy selects the action that maximizes the q-value function at any given state.\n",
    "\n",
    "#### ðŸ‘‰In practice you can pick an epsilon-greedy policy, to avoid overfitting issues.\n",
    "\n",
    "#### ðŸ‘‰Don't know what I am talking about? Read [part 2](https://towardsdatascience.com/hands-on-reinforcement-learning-course-part-2-1b0828a1046b) of the course."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ae8a8539",
   "metadata": {},
   "outputs": [],
   "source": [
    "from src.viz import plot_policy\n",
    "\n",
    "positions = np.arange(env.min_position, env.max_position, 0.05)\n",
    "velocities = np.arange(-env.max_speed, env.max_speed, 0.005)\n",
    "\n",
    "plot_policy(agent, positions, velocities)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bac8fd1f",
   "metadata": {},
   "source": [
    "## Save the agent to disk ðŸ’¾"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7b065eb3",
   "metadata": {},
   "outputs": [],
   "source": [
    "from src.config import SAVED_AGENTS_DIR\n",
    "path = SAVED_AGENTS_DIR / 'sarsa_agent_10k_steps'\n",
    "print(f'Saving agent to {path}')\n",
    "agent.save_to_disk(path)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d8e41960",
   "metadata": {},
   "source": [
    "## Let's see our agent in action ðŸŽ¬"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5c742634",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Workaround for pygame error: \"error: No available video device\"\n",
    "# See https://stackoverflow.com/questions/15933493/pygame-error-no-available-video-device?rq=1\n",
    "# This is probably needed only for Linux\n",
    "import os\n",
    "os.environ[\"SDL_VIDEODRIVER\"] = \"dummy\"\n",
    "from src.viz import show_video\n",
    "\n",
    "env = gym.make('MountainCar-v0', max_episode_steps=1000, render_mode='rgb_array')\n",
    "show_video(agent, env, sleep_sec=0.01)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fa049525",
   "metadata": {},
   "source": [
    "## And plot the policy ðŸŽ¨"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "55e63892",
   "metadata": {},
   "outputs": [],
   "source": [
    "from src.viz import plot_policy\n",
    "\n",
    "positions = np.arange(env.min_position, env.max_position, 0.05)\n",
    "velocities = np.arange(-env.max_speed, env.max_speed, 0.005)\n",
    "sarsa_policy = plot_policy(agent, positions, velocities)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "699e402a",
   "metadata": {},
   "source": [
    "## This is clearly different than the best one, from the MomentumAgent"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "410befa1",
   "metadata": {},
   "outputs": [],
   "source": [
    "from src.momentum_agent import MomentumAgent\n",
    "\n",
    "perfect_agent = MomentumAgent(env)\n",
    "perfect_policy = plot_policy(perfect_agent, positions, velocities)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a683f79a",
   "metadata": {},
   "source": [
    "## Actually, they overlap only in 50% of the states"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "42943841",
   "metadata": {
    "lines_to_next_cell": 0
   },
   "outputs": [],
   "source": [
    "perfect_action = perfect_policy['action']\n",
    "sarsa_action = sarsa_policy['action']\n",
    "\n",
    "(perfect_action == sarsa_action).sum() / len(perfect_action)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6dab2d50",
   "metadata": {
    "lines_to_next_cell": 2
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "src-a5jWQe4R-py3.12",
   "language": "python",
   "name": "python3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
