{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "71fefbce",
   "metadata": {},
   "source": [
    "# 00 Environment"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5bbfcc6c",
   "metadata": {},
   "source": [
    "#### ðŸ‘‰Before you solve a Reinforcement Learning problem you need to define what are\n",
    "- the actions\n",
    "- the states of the world\n",
    "- the rewards\n",
    "\n",
    "#### ðŸ‘‰We are using the `CartPole-v0` environment from [OpenAI's gym](https://github.com/openai/gym/blob/master/gym/envs/classic_control/cartpole.py)\n",
    "\n",
    "#### ðŸ‘‰`CartPole-v0` is not an extremely difficult environment. However, it is complex enough to force us level up our game. The tools we will use to solve it are really powerful.\n",
    "\n",
    "#### ðŸ‘‰Let's explore it!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8d84a023",
   "metadata": {},
   "outputs": [],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2\n",
    "%pylab inline\n",
    "%config InlineBackend.figure_format = 'svg'\n",
    "\n",
    "from matplotlib import pyplot as plt\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "888c9e82",
   "metadata": {},
   "source": [
    "## Load the environment ðŸŒŽ"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "442988fe",
   "metadata": {},
   "outputs": [],
   "source": [
    "import gymnasium as gym\n",
    "env = gym.make('CartPole-v1')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "684a6a8d",
   "metadata": {},
   "source": [
    "## The goal\n",
    "### is to keep the pole in an upright position as long as you can by moving the cart a the bottom, left and right."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ac8008d3",
   "metadata": {},
   "source": [
    "![title](../images/cart_pole.jpg)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "12d03c50",
   "metadata": {},
   "source": [
    "## Let's see how a good agent solves this problem"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "57f2a5c1",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "914ca8dd",
   "metadata": {
    "lines_to_end_of_cell_marker": 2
   },
   "outputs": [],
   "source": [
    "# env.reset()\n",
    "# frame = env.render(mode='rgb_array')\n",
    "\n",
    "# fig, ax = plt.subplots(figsize=(8, 6))\n",
    "# ax.axes.yaxis.set_visible(False)\n",
    "# min_x = env.observation_space.low[0]\n",
    "# max_x = env.observation_space.high[0]\n",
    "# ax.imshow(frame, extent=[min_x, max_x, 0, 8])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "10fc3ab1",
   "metadata": {},
   "source": [
    "## State space"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "940f2742",
   "metadata": {},
   "outputs": [],
   "source": [
    "# The state consists of 4 numbers:\n",
    "x_min, v_min, angle_min, angular_v_min = env.observation_space.low\n",
    "x_max, v_max, angle_max, angular_v_max = env.observation_space.high\n",
    "\n",
    "print(f'Cart position from {x_min:.2f} to {x_max:.2f}')\n",
    "print(f'Cart velocity from {v_min:.2E} to {v_max:.2E}')\n",
    "print(f'Angle from {angle_min:.2f} to {angle_max:.2f}')\n",
    "print(f'Angular velocity from {angular_v_min:.2E} to {angular_v_max:.2E}')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "47e21842",
   "metadata": {},
   "source": [
    "[IMAGE]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6a35768c",
   "metadata": {},
   "source": [
    "### The ranges for the cart velocity and pole angular velocity are a bit too large, aren't they?\n",
    "\n",
    "ðŸ‘‰ As a general principle, the high/low state values you can read from `env.observation_space`\n",
    "are set very conservatively, to guarantee that the state value alwayas lies between the max and the min.\n",
    "\n",
    "ðŸ‘‰In practice, you need to simulate a few interactions with the environment to really see the actual intervals where the state components lie.\n",
    "\n",
    "ðŸ‘‰ Knowing the max and min values for each state component is going to be useful later when we normalize the inputs to our Parametric models."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1e155362",
   "metadata": {},
   "source": [
    "## Action space\n",
    "\n",
    "- `0` Push cart to the left\n",
    "- `1` Push cart to the right"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e11d68c1",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Action Space {}\".format(env.action_space))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b088e5a5",
   "metadata": {},
   "source": [
    "## Rewards\n",
    "\n",
    "- A reward of -1 is awarded if the position of the car is less than 0.5.\n",
    "- The episode ends once the car's position is above 0.5, or the max number of steps has been reached: `n_steps >= env._max_episode_steps`\n",
    "\n",
    "A default negative reward of -1 encourages the car to escape the valley as fast as possible."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "69cc8e76",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
