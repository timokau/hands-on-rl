{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "807e2c7f",
   "metadata": {},
   "source": [
    "# 09 Hyper-parameter search for Deep Q Learning\n",
    "\n",
    "#### üëâDeep RL is hard, because (among other things) it's very sensitivity to the hyper-parameters.\n",
    "\n",
    "#### üëâWe tune the hyper-parmeters following a trial&error approach:\n",
    "\n",
    "![](../images/hparams_search_diagram.svg)\n",
    "\n",
    "#### üëâHowever, Hyper-parameter spaces in deep RL problems are HUGE. A brute-force solution that would try all possible combinations of hyper-parameters is not feasible. We need something smarter than that...\n",
    "\n",
    "#### üëâAnd this is when Bayesian search methods enther into the picture.\n",
    "\n",
    "#### üëâIn a nutshell, Bayesian search methods use past searches to inform promising avenues.\n",
    "\n",
    "#### üëâ [Optuna](https://optuna.readthedocs.io/en/stable/index.html) is a Python open-source library that implements Bayesian search methods\n",
    "\n",
    "<img src=\"https://github.com/Paulescu/hands-on-rl/blob/main/03_cart_pole/images/optuna.png?raw=True\" width=\"400\"/>\n",
    "\n",
    "#### üëâHyper-paramater search a piece of cake üç∞if you use Optuna."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "38f0ab23",
   "metadata": {},
   "outputs": [],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2\n",
    "%pylab inline\n",
    "%config InlineBackend.figure_format = 'svg'"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2f8cffe9",
   "metadata": {},
   "source": [
    "## Environment üåé"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5e93e313",
   "metadata": {},
   "outputs": [],
   "source": [
    "import gymnasium as gym\n",
    "env = gym.make('CartPole-v1')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8bb226ce",
   "metadata": {},
   "source": [
    "### MLflow is a useful tool to track experiment results\n",
    "\n",
    "cd to the root directory of this lesson (in my case `/Users/paulabartabajo/src/online-courses/hands-on-rl/03_cart_pole`) and spin up the mlflow tracking server as follows:\n",
    "\n",
    "**$ mlflow server --backend-store-uri sqlite:///mlflow.db --default-artifact-root ./artifacts --host 0.0.0.0 --port 5000**\n",
    "\n",
    "### üí° if you have another service listening to port 5000, increase port number by 1 until you hit a free port."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "be05b289",
   "metadata": {},
   "outputs": [],
   "source": [
    "import mlflow\n",
    "\n",
    "# connect mlflow client to the mlflow server that runs on localhost:5000\n",
    "MLFLOW_SERVER_URI = 'http://localhost:5000'\n",
    "mlflow.set_tracking_uri(str(MLFLOW_SERVER_URI))\n",
    "\n",
    "EXPERIMENT_NAME = 'hyperparameter_search'\n",
    "mlflow.set_experiment(EXPERIMENT_NAME)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a0af33db",
   "metadata": {},
   "source": [
    "## Create an Optuna study"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0bda3ff2",
   "metadata": {},
   "outputs": [],
   "source": [
    "import optuna\n",
    "\n",
    "from src.config import OPTUNA_DB\n",
    "\n",
    "study = optuna.create_study(\n",
    "    study_name=EXPERIMENT_NAME,\n",
    "    direction='maximize',\n",
    "    load_if_exists=True,\n",
    "    storage=f'sqlite:///{OPTUNA_DB}'\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "38cbec7c",
   "metadata": {},
   "source": [
    "## Objective function we want to maximize"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "485a3e18",
   "metadata": {},
   "outputs": [],
   "source": [
    "from src.optimize_hyperparameters import objective\n",
    "\n",
    "# we define a lambda function because study.optimize()\n",
    "# expect the objective function to have only 1 input\n",
    "# (trial), while our objective function hast 2 extra\n",
    "# inputs I defined to add flexibility to the script\n",
    "func = lambda trial: objective(trial,\n",
    "                               force_linear_model=False,\n",
    "                               n_episodes_to_train=200)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "70dee19a",
   "metadata": {},
   "source": [
    "## Set threshold to terminate hyperparameter search"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "beb3962f",
   "metadata": {},
   "outputs": [],
   "source": [
    "class CheckHyperparamMeanRewardThreshold:\n",
    "    def __init__(self, reward_threshold: float):\n",
    "        self.reward_threshold = reward_threshold\n",
    "\n",
    "    def __call__(self, study: optuna.study.Study, trial: optuna.trial.FrozenTrial) -> None:\n",
    "        if trial.value >= self.reward_threshold:\n",
    "            print((f'Stopping hyperparameter search because trial.value ({trial.value}) '\n",
    "                   f'hit threshold ({self.reward_threshold})'))\n",
    "            study.stop()\n",
    "\n",
    "# Stop hyperparameter search when we hit a perfect mean reward of 500\n",
    "hyperparam_search_stop_callback = CheckHyperparamMeanRewardThreshold(500.0)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0bfd36fa",
   "metadata": {},
   "source": [
    "## Let's start the search"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9ac27bd5",
   "metadata": {},
   "outputs": [],
   "source": [
    "study.optimize(func, n_trials=1000, callbacks=[hyperparam_search_stop_callback])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "947669cf",
   "metadata": {},
   "source": [
    "## These are the best hyper-parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a92ea5ae",
   "metadata": {},
   "outputs": [],
   "source": [
    "best_trial = study.best_trial\n",
    "\n",
    "hparams = {k: best_trial.params[k] for k in best_trial.params if k != 'seed'}\n",
    "#hparams['nn_hidden_layers'] = eval(hparams['nn_hidden_layers']) \n",
    "print(hparams)\n",
    "\n",
    "SEED = best_trial.params['seed']\n",
    "print('Seed: ', SEED)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6bcbf2a2",
   "metadata": {},
   "source": [
    "## We can re-run the training to get the perfect agent"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "965342db",
   "metadata": {},
   "outputs": [],
   "source": [
    "from src.utils import set_seed\n",
    "set_seed(env, SEED)\n",
    "\n",
    "from src.q_agent import QAgent\n",
    "agent = QAgent(env, **hparams)\n",
    "\n",
    "from src.loops import train\n",
    "train(agent, env, n_episodes=200)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8a153957",
   "metadata": {},
   "source": [
    "## or simply load the `agent_id` from the best run"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "21d94196",
   "metadata": {},
   "outputs": [],
   "source": [
    "from src.q_agent import QAgent\n",
    "from src.config import SAVED_AGENTS_DIR\n",
    "\n",
    "# you can find the agent_id for the best run in the MLflow\n",
    "# dashboard.\n",
    "# 298 is the value in my case, but you need to check what is your\n",
    "agent_id = 298\n",
    "\n",
    "path_to_saved_model = SAVED_AGENTS_DIR / 'CartPole-v1' / str(agent_id)\n",
    "agent = QAgent.load_from_disk(env, path_to_saved_model)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "37a963ab",
   "metadata": {},
   "source": [
    "## Evaluate the agent ‚è±Ô∏è"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1205b029",
   "metadata": {},
   "outputs": [],
   "source": [
    "from src.loops import evaluate\n",
    "rewards, steps = evaluate(\n",
    "    agent, env,\n",
    "    n_episodes=1000,\n",
    "    epsilon=0.00\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "005d9d19",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "reward_avg = np.array(rewards).mean()\n",
    "reward_std = np.array(rewards).std()\n",
    "print(f'Reward average {reward_avg:.2f}, std {reward_std:.2f}')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "abf148cd",
   "metadata": {},
   "source": [
    "## Let's see how far we got in each attempt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "44eae10e",
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import pandas as pd\n",
    "\n",
    "fig, ax = plt.subplots(figsize = (10, 4))\n",
    "ax.set_title(\"Rewards\")    \n",
    "pd.Series(rewards).plot(kind='hist', bins=100)\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1ec53bed",
   "metadata": {},
   "source": [
    "## Let's see our agent in action üé¨"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "36af0161",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Workaround for pygame error: \"error: No available video device\"\n",
    "# See https://stackoverflow.com/questions/15933493/pygame-error-no-available-video-device?rq=1\n",
    "# This is probably needed only for Linux\n",
    "import os\n",
    "os.environ[\"SDL_VIDEODRIVER\"] = \"dummy\"\n",
    "\n",
    "from src.viz import show_video\n",
    "\n",
    "show_video(agent, env, sleep_sec=0.01, seed=123)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
