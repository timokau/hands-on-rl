{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "c23af216",
   "metadata": {},
   "source": [
    "# 02 Q-agent"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7e564885",
   "metadata": {},
   "source": [
    "#### ðŸ‘‰Let's use Q-learning to train a smart taxi driver.\n",
    "\n",
    "#### ðŸ‘‰A smart taxi driver should incur no penalties (i.e. no crashes) and minimize the timesteps to complete the ride."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e8e729eb",
   "metadata": {},
   "outputs": [],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2\n",
    "%pylab inline\n",
    "%config InlineBackend.figure_format = 'svg'"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ded2597d",
   "metadata": {},
   "source": [
    "## Environment ðŸŒŽ"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5b6b8d78",
   "metadata": {},
   "outputs": [],
   "source": [
    "import gymnasium as gym\n",
    "env = gym.make(\"Taxi-v3\", render_mode=\"ansi\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0f4953da",
   "metadata": {},
   "source": [
    "## Q-agent ðŸ¤–ðŸ§ "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d9e0c065",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "class QAgent:\n",
    "\n",
    "    def __init__(self, env, alpha, gamma):\n",
    "        self.env = env\n",
    "\n",
    "        # table with q-values: n_states * n_actions\n",
    "        self.q_table = np.zeros([env.observation_space.n,\n",
    "                                 env.action_space.n])\n",
    "\n",
    "        # hyper-parameters\n",
    "        self.alpha = alpha  # learning rate\n",
    "        self.gamma = gamma  # discount factor\n",
    "\n",
    "    def get_action(self, state):\n",
    "        \"\"\"\"\"\"\n",
    "        return np.argmax(self.q_table[state])\n",
    "\n",
    "    def update_parameters(self, state, action, reward, next_state):\n",
    "        \"\"\"\"\"\"\n",
    "        # Q-learning formula\n",
    "        old_value = self.q_table[state, action]\n",
    "        next_max = np.max(self.q_table[next_state])\n",
    "        new_value = \\\n",
    "            old_value + \\\n",
    "            self.alpha * (reward + self.gamma * next_max - old_value)\n",
    "\n",
    "        # update the q_table\n",
    "        self.q_table[state, action] = new_value"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c29e5daf",
   "metadata": {},
   "outputs": [],
   "source": [
    "# hyper-parameters\n",
    "# RL problems are full of these hyper-parameters.\n",
    "# For the moment, trust me when I set these values.\n",
    "# We will later play with these and see how they impact learning.\n",
    "alpha = 0.1\n",
    "gamma = 0.6\n",
    "\n",
    "agent = QAgent(env, alpha, gamma)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "05b890b7",
   "metadata": {},
   "source": [
    "## Training loop ðŸŽ¡"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6e554a27",
   "metadata": {},
   "outputs": [],
   "source": [
    "import random\n",
    "from tqdm import tqdm\n",
    "\n",
    "# exploration vs exploitation prob\n",
    "epsilon = 0.1\n",
    "\n",
    "n_episodes = 10000\n",
    "\n",
    "# For plotting metrics\n",
    "timesteps_per_episode = []\n",
    "penalties_per_episode = []\n",
    "\n",
    "\n",
    "for i in tqdm(range(0, n_episodes)):\n",
    "    \n",
    "    state, _ = env.reset()\n",
    "\n",
    "    epochs, penalties, reward, = 0, 0, 0\n",
    "    done = False\n",
    "    \n",
    "    while not done:\n",
    "        \n",
    "        if random.uniform(0, 1) < epsilon:\n",
    "            # Explore action space\n",
    "            action = env.action_space.sample()\n",
    "        else:\n",
    "            # Exploit learned values\n",
    "            action = agent.get_action(state)\n",
    "        \n",
    "        next_state, reward, terminated, truncated, info = env.step(action)\n",
    "        done = terminated or truncated\n",
    "        \n",
    "        agent.update_parameters(state, action, reward, next_state)\n",
    "        \n",
    "        if reward == -10:\n",
    "            penalties += 1\n",
    "\n",
    "        state = next_state\n",
    "        epochs += 1\n",
    "        \n",
    "    timesteps_per_episode.append(epochs)\n",
    "    penalties_per_episode.append(penalties)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "916799ea",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "fig, ax = plt.subplots(figsize = (12, 4))\n",
    "ax.set_title(\"Timesteps to complete ride\")    \n",
    "pd.Series(timesteps_per_episode).plot(kind='line')\n",
    "plt.show()\n",
    "\n",
    "fig, ax = plt.subplots(figsize = (12, 4))\n",
    "ax.set_title(\"Penalties per ride\")    \n",
    "pd.Series(penalties_per_episode).plot(kind='line')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a96da26c",
   "metadata": {},
   "source": [
    "## Let's evaluate this driver starting from a fixed `state = 123`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a6f68ff7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# set initial state of the environment\n",
    "state, _ = env.reset(seed=123)\n",
    "\n",
    "epochs = 0\n",
    "penalties = 0\n",
    "reward = 0\n",
    "\n",
    "# store frames to latter plot them\n",
    "frames = []\n",
    "\n",
    "done = False\n",
    "\n",
    "while not done:\n",
    "    \n",
    "    action = agent.get_action(state)\n",
    "    \n",
    "    next_state, reward, terminated, truncated, info = env.step(action)\n",
    "    done = terminated or truncated\n",
    "\n",
    "    if reward == -10:\n",
    "        penalties += 1\n",
    "    \n",
    "    frames.append({\n",
    "        'frame': env.render(),\n",
    "        'state': state,\n",
    "        'action': action,\n",
    "        'reward': reward\n",
    "        }\n",
    "    )\n",
    "\n",
    "    state = next_state\n",
    "\n",
    "    epochs += 1\n",
    "    \n",
    "print(\"Timesteps taken: {}\".format(epochs))\n",
    "print(\"Penalties incurred: {}\".format(penalties))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d9744037",
   "metadata": {},
   "outputs": [],
   "source": [
    "from IPython.display import clear_output\n",
    "from time import sleep\n",
    "\n",
    "def print_frames(frames):\n",
    "    for i, frame in enumerate(frames):\n",
    "        clear_output(wait=True)\n",
    "        print(frame['frame'])\n",
    "        print(f\"Timestep: {i + 1} of {len(frames)}\")\n",
    "        print(f\"State: {frame['state']}\")\n",
    "        print(f\"Action: {frame['action']}\")\n",
    "        print(f\"Reward: {frame['reward']}\")\n",
    "        sleep(.1)\n",
    "        \n",
    "print_frames(frames)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dd1225b8",
   "metadata": {},
   "source": [
    "## Let's evaluate this trained agent on 100 episodes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e4a81648",
   "metadata": {},
   "outputs": [],
   "source": [
    "import random\n",
    "from tqdm import tqdm\n",
    "\n",
    "# exploration vs exploitation prob\n",
    "epsilon = 0.05\n",
    "\n",
    "n_episodes = 100\n",
    "\n",
    "# For plotting metrics\n",
    "timesteps_per_episode = []\n",
    "penalties_per_episode = []\n",
    "\n",
    "\n",
    "for i in tqdm(range(0, n_episodes)):\n",
    "    \n",
    "    state, _ = env.reset()      \n",
    "    \n",
    "    epochs, penalties, reward, = 0, 0, 0\n",
    "    done = False\n",
    "    \n",
    "    while not done:\n",
    "        \n",
    "        if random.uniform(0, 1) < epsilon:\n",
    "            # Explore action space\n",
    "            action = env.action_space.sample()\n",
    "        else:\n",
    "            # Exploit learned values\n",
    "            action = agent.get_action(state)\n",
    "        \n",
    "        next_state, reward, terminated, truncated, info = env.step(action)\n",
    "        done = terminated or truncated\n",
    "                              \n",
    "        if reward == -10:\n",
    "            penalties += 1\n",
    "\n",
    "        state = next_state\n",
    "        epochs += 1\n",
    "            \n",
    "    timesteps_per_episode.append(epochs)\n",
    "    penalties_per_episode.append(penalties)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "aa98a04b",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(f'Avg steps to complete ride: {np.array(timesteps_per_episode).mean()}')\n",
    "print(f'Avg penalties to complete ride: {np.array(penalties_per_episode).mean()}')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv",
   "language": "python",
   "name": "python3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
