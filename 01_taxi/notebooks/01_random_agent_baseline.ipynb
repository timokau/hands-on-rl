{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "74aa48d4",
   "metadata": {},
   "source": [
    "# 01 Random agent baseline"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ef302f41",
   "metadata": {},
   "source": [
    "#### ðŸ‘‰Before you try to solve a Reinforcement Learning problem you should get a grasp of its difficulty.\n",
    "\n",
    "#### ðŸ‘‰ To do so, you need to design a dummy agent that can peform the task without much brains, and evaluate its performance.\n",
    "\n",
    "#### ðŸ‘‰A simple way to do so is by using a Random Agent, that chooses its next action randomly, without paying attention at the current state of the environment.\n",
    "\n",
    "#### ðŸ‘‰Needless to say, do not grab a taxi driven by a Random Driver."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1623ba3e",
   "metadata": {},
   "outputs": [],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2\n",
    "%pylab inline\n",
    "%config InlineBackend.figure_format = 'svg'"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8bc37d4d",
   "metadata": {},
   "source": [
    "## Environment ðŸŒŽ"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "48b4f6e6",
   "metadata": {},
   "outputs": [],
   "source": [
    "import gymnasium as gym\n",
    "env = gym.make(\"Taxi-v3\", render_mode=\"ansi\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "637a9f60",
   "metadata": {},
   "source": [
    "## Random agent ðŸ¤–ðŸ·"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "41f50b2c",
   "metadata": {},
   "outputs": [],
   "source": [
    "class RandomAgent:\n",
    "    \"\"\"\n",
    "    This taxi driver selects actions randomly.\n",
    "    You better not get into this taxi!\n",
    "    \"\"\"\n",
    "    def __init__(self, env):\n",
    "        self.env = env\n",
    "\n",
    "    def get_action(self, state) -> int:\n",
    "        \"\"\"\n",
    "        We have `state` as an input to keep\n",
    "        a consistent API for all our agents, but it\n",
    "        is not used.\n",
    "        \n",
    "        i.e. The agent does not consider the state of\n",
    "        the environment when deciding what to do next.\n",
    "        This is why we call it \"random\".\n",
    "        \"\"\"\n",
    "        return self.env.action_space.sample()\n",
    "\n",
    "agent = RandomAgent(env)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "52c78105",
   "metadata": {},
   "source": [
    "## Let's evaluate this driver starting from a fixed `state = 123`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9965b4b0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# set initial state of the environment\n",
    "state, _ = env.reset(seed=123)\n",
    "\n",
    "epochs = 0\n",
    "penalties = 0  # wrong pick up or dropp off\n",
    "reward = 0\n",
    "\n",
    "# store frames to latter plot them\n",
    "frames = []\n",
    "\n",
    "done = False\n",
    "\n",
    "while not done:\n",
    "    \n",
    "    action = agent.get_action(state)\n",
    "    \n",
    "    state, reward, terminated, truncated, info = env.step(action)\n",
    "    done = terminated or truncated\n",
    "\n",
    "    if reward == -10:\n",
    "        penalties += 1\n",
    "    \n",
    "    frames.append({\n",
    "        'frame': env.render(),\n",
    "        'state': state,\n",
    "        'action': action,\n",
    "        'reward': reward\n",
    "        }\n",
    "    )\n",
    "\n",
    "    epochs += 1\n",
    "    \n",
    "    \n",
    "print(\"Timesteps taken: {}\".format(epochs))\n",
    "print(\"Penalties incurred: {}\".format(penalties))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "29632c3c",
   "metadata": {},
   "outputs": [],
   "source": [
    "from IPython.display import clear_output\n",
    "from time import sleep\n",
    "\n",
    "def print_frames(frames):\n",
    "    for i, frame in enumerate(frames):\n",
    "        clear_output(wait=True)\n",
    "        print(frame['frame'])\n",
    "        print(f\"Timestep: {i + 1} of {len(frames)}\")\n",
    "        print(f\"State: {frame['state']}\")\n",
    "        print(f\"Action: {frame['action']}\")\n",
    "        print(f\"Reward: {frame['reward']}\")\n",
    "        sleep(.01)\n",
    "        \n",
    "print_frames(frames)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8054e101",
   "metadata": {},
   "source": [
    "#### Pretty bad driving, isn't it?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "18a5d816",
   "metadata": {},
   "source": [
    "## Let's generate histograms to quantify performance"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ab7e219a",
   "metadata": {},
   "outputs": [],
   "source": [
    "from tqdm import tqdm\n",
    "\n",
    "n_episodes = 100\n",
    "\n",
    "# For plotting metrics\n",
    "timesteps_per_episode = []\n",
    "penalties_per_episode = []\n",
    "\n",
    "for i in tqdm(range(0, n_episodes)):\n",
    "    \n",
    "    # reset environment to a random state\n",
    "    state, _ = env.reset()\n",
    "\n",
    "    epochs, penalties, reward, = 0, 0, 0\n",
    "    done = False\n",
    "    \n",
    "    while not done:\n",
    "        \n",
    "        action = agent.get_action(state)       \n",
    "        next_state, reward, terminated, truncated, info = env.step(action)\n",
    "        done = terminated or truncated\n",
    "               \n",
    "        if reward == -10:\n",
    "            penalties += 1\n",
    "\n",
    "        state = next_state\n",
    "        epochs += 1\n",
    "    \n",
    "    timesteps_per_episode.append(epochs)\n",
    "    penalties_per_episode.append(penalties)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6b370999",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "fig, ax = plt.subplots(figsize = (12, 4))\n",
    "ax.set_title(\"Timesteps to complete ride\")    \n",
    "pd.Series(timesteps_per_episode).plot(kind=\"line\")\n",
    "plt.show()\n",
    "\n",
    "fig, ax = plt.subplots(figsize = (12, 4))\n",
    "ax.set_title(\"Penalties per ride\")    \n",
    "pd.Series(penalties_per_episode).plot(kind=\"line\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3529294b",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(f'Avg steps to complete ride: {np.array(timesteps_per_episode).mean()}')\n",
    "print(f'Avg penalties to complete ride: {np.array(penalties_per_episode).mean()}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c996034b",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv",
   "language": "python",
   "name": "python3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
