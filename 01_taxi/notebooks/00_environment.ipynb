{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "46414ad0",
   "metadata": {},
   "source": [
    "# 00 Environment"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f8211107",
   "metadata": {},
   "source": [
    "#### ðŸ‘‰Before you solve a Reinforcement Learning problem you need to define what are\n",
    "- the actions\n",
    "- the states of the world\n",
    "- the rewards\n",
    "\n",
    "#### ðŸ‘‰We are using the `Taxi-v3` environment from OpenAI's gym: https://gym.openai.com/envs/Taxi-v3/\n",
    "\n",
    "#### ðŸ‘‰`Taxi-v3` is an easy environment because the action space is small, and the state space is large but finite.\n",
    "\n",
    "#### ðŸ‘‰Environments with a finite number of actions and states are called tabular"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "756294bf",
   "metadata": {},
   "outputs": [],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2\n",
    "%pylab inline\n",
    "%config InlineBackend.figure_format = 'svg'"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1c5caa97",
   "metadata": {},
   "source": [
    "## Load the environment ðŸŒŽ"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a57c5d9d",
   "metadata": {},
   "outputs": [],
   "source": [
    "import gymnasium as gym\n",
    "env = gym.make(\"Taxi-v3\", render_mode=None)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6e05d6b2",
   "metadata": {},
   "source": [
    "## Action space"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b34b25f8",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Action Space {}\".format(env.action_space))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d51940b9",
   "metadata": {},
   "source": [
    "## State space"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f8b4bce0",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"State Space {}\".format(env.observation_space))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c22347fb",
   "metadata": {},
   "source": [
    "## Rewards"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8d507154",
   "metadata": {},
   "outputs": [],
   "source": [
    "# env.P is double dictionary.\n",
    "# - The 1st key represents the state, from 0 to 499\n",
    "# - The 2nd key represens the action taken by the agent,\n",
    "#   from 0 to 5\n",
    "\n",
    "# example\n",
    "state = 123\n",
    "action = 0  # move south\n",
    "\n",
    "# env.P[state][action][0] is a list with 4 elements\n",
    "# (probability, next_state, reward, done)\n",
    "# \n",
    "#  - probability\n",
    "#    It is always 1 in this environment, which means\n",
    "#    there are no external/random factors that determine the\n",
    "#    next_state\n",
    "#    apart from the agent's action a.\n",
    "#\n",
    "#  - next_state: 223 in this case\n",
    "# \n",
    "#  - reward: -1 in this case\n",
    "#\n",
    "#  - done: boolean (True/False) indicates whether the\n",
    "#    episode has ended (i.e. the driver has dropped the\n",
    "#    passenger at the correct destination)\n",
    "print('env.P[state][action][0]: ', env.P[state][action][0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1c227798",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Need to call reset() at least once before render() will work\n",
    "env.reset()\n",
    "\n",
    "env.reset(seed=123)\n",
    "env.render()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8d1f53bb",
   "metadata": {},
   "outputs": [],
   "source": [
    "env.reset(seed=223)\n",
    "env.render()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "00839b95",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
